# figure out how to get back admin key via get token
# send the request you get when scrolling to get the test response in json format
# extract da data

from collections import defaultdict
import requests
import brotli
import zlib
import random
import aiohttp
import asyncio
import sys
import time

MAX_RETRIES = 5
RETRYABLE_CODES = [429, 500, 502, 503, 504, 531]
# Step 1: Get the token

def process_car_data(cars):
    # Initialize variables for max, min, total price, and mode make count
    max_year = -float('inf')
    min_year = float('inf')
    total_price = 0
    make_counts = defaultdict(int)  # To count occurrences of each make
    
    for car in cars:
        # Process year
        # print(car['year'])
        year = int(car['year'])
        max_year = max(max_year, int(year))
        min_year = min(min_year, int(year))
        
        # Process price
        total_price += car['price']
        
        # Process make (for mode)
        make_counts[car['make']] += 1

    # Calculate average price
    avg_price = total_price / len(cars)
    
    # Find mode make (most common make)
    mode_make = max(make_counts, key=make_counts.get)
    
    # Return the results
    return max_year, min_year, avg_price, mode_make

proxies = ['http://pingproxies:scrapemequickly@194.87.135.1:9875',
'http://pingproxies:scrapemequickly@194.87.135.2:9875',
'http://pingproxies:scrapemequickly@194.87.135.3:9875',
'http://pingproxies:scrapemequickly@194.87.135.4:9875',
'http://pingproxies:scrapemequickly@194.87.135.5:9875']

def get_random_proxy(batch_index):
    return {'http': proxies[batch_index % len(proxies)], 'https': proxies[batch_index % len(proxies)]}

url = "https://api.scrapemequickly.com/get-token"
params = {
    "scraping_run_id": "89d5dca4-0a34-11f0-b686-4a33b21d14f6"
}
headers = {
    "Accept": "*/*",
    "Sec-Fetch-Site": "same-site",
    "Origin": "https://scrapemequickly.com",
    "Sec-Fetch-Dest": "empty",
    "Accept-Language": "en-GB,en;q=0.9",
    "Sec-Fetch-Mode": "cors",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.3 Safari/605.1.15",
    "Accept-Encoding": "gzip, deflate, br",
    "Referer": "https://scrapemequickly.com/",
    "Priority": "u=3, i"
}

response = requests.get(url, headers=headers, params=params)

tokendata = response.json()
token = tokendata['token']

# URL for the actual GET request
url = "https://api.scrapemequickly.com/cars/test"

# Headers including the Authorization token
headers = {
    "Authorization": f"Bearer {token}",
    "Accept": "*/*",
    "Sec-Fetch-Mode": "cors",
    "Sec-Fetch-Site": "same-site",
    "Sec-Fetch-Dest": "empty",
    "Origin": "https://scrapemequickly.com",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "en-GB,en-US;q=0.9,en;q=0.8",
    "Referer": "https://scrapemequickly.com/",
    "Priority": "u=1, i"
}
data = []
MAX_RETRIES = 5
RETRYABLE_CODES = [429, 500, 502, 503, 504, 531]

data = []

for x in range(1, 4000):
    params = {
        "scraping_run_id": "89d5dca4-0a34-11f0-b686-4a33b21d14f6",
        "per_page": 25,
        "start": 25 * x
    }

    retries = 0
    success = False

    while retries < MAX_RETRIES and not success:
        try:
            response = requests.get(
                url,
                headers=headers,
                params=params,
                proxies=get_random_proxy(x),
                timeout=10  # Timeout just in case
            )

            if response.status_code == 200:
                json_data = response.json()
                if not json_data['data']:
                    print("Empty data at page", x)
                    break  # Stop scraping if we hit the end
                data.extend(json_data['data'])  # Extend, not append!
                print(f"Fetched page {x}")
                success = True

            elif response.status_code in RETRYABLE_CODES:
                print(f"Retryable error {response.status_code} on page {x}, attempt {retries+1}")
                retries += 1
                time.sleep(random.uniform(1.5, 3.5))  # Exponential-ish backoff
            else:
                print(f"Non-retryable error {response.status_code} on page {x}")
                print("Response:", response.text)
                break  # Don't retry non-retryable errors

        except requests.exceptions.RequestException as e:
            print(f"Request failed on page {x}: {e}")
            retries += 1
            time.sleep(random.uniform(1, 3))

    if not success:
        print(f"Failed to fetch page {x} after {MAX_RETRIES} retries.")
        continue  # Move on to the next page



max_year, min_year, avg_price, mode_make = process_car_data(data)

def start_scraping_run(team_id: str) -> str:
    r = requests.post(f"https://api.scrapemequickly.com/scraping-run?team_id={"dff0f82d-1241-11f0-8d9a-0242ac120003"}")
    
    if r.status_code != 200:
        print(r.json())
        print("Failed to start scraping run")
        sys.exit(1)

    return r.json()["data"]["scraping_run_id"]

# Output the results
print(f"Max Year: {max_year}")
print(f"Min Year: {min_year}")
print(f"Average Price: {avg_price:.2f}")
print(f"Most Common Make: {mode_make}")